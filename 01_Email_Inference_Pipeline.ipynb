{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML - Sample Batch Prediction Pipeline\n",
    "- No parallel run step leveraged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates creation and execution of an Azure ML pipeline designed to load data from a remote source, to make predictions against that data using a previously registered ML model, and finally save that data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = 'email-classification-inference-pipeline'\n",
    "cluster_name = \"cpu-cluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData, PipelineEndpoint\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create a folder for the pipeline step files\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to inference, we need a dataset to inference on, so we will load into the inference folder location our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "default_ds.upload_files(files=['./datasets/spaminference.csv'], # Upload the diabetes csv files in /data\n",
    "                        target_path= 'spam-data', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "    \n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'spam-data/spaminference.csv'))\n",
    "\n",
    "# Display the first 20 rows as a Pandas dataframe\n",
    "tab_data_set.take(20).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "print('trying to create: ' + cluster_name)\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2, idle_seconds_before_scaledown=1800)\n",
    "        compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        compute_target.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Configuration\n",
    "Create configuration for the running pipeline.  The RunConfiguration defines the environment used in the python steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn','ipykernel','matplotlib','pandas','pip'],\n",
    "                                                                            pip_packages=['azureml-sdk','numpy', 'joblib', 'sklearn' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Output Datasets\n",
    "\n",
    "Below are the configuration for datasets that will be passed between steps in our pipelien.  Note, in all cases we specifiy the datastore that should hold the datasets and wheather they should be registered following step completion or not.  This can optionally be disabled by removing the register_on_complete() call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencing_dataset = OutputFileDatasetConfig(name= 'email_inferencing_dataset', destination=(default_ds, 'inferencing_dataset/{run-id}')).read_delimited_files().register_on_complete(name= 'email_inferencing_data')\n",
    "scored_dataset      = OutputFileDatasetConfig(name= 'email_scored_dataset', destination=(default_ds, 'scored_dataset/{run-id}')).read_delimited_files().register_on_complete(name= 'email_scored_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pipeline Parameters\n",
    "\n",
    "PipelineParameter objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Multiple pipeline parameters can be created and used. Included here are multiple sample pipeline parameters (get_data_param_*) to highlight how parameters can be passed into and consumed by various pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data_location_parm = PipelineParameter(name='inference_data_location', default_value= 'spam-data')\n",
    "model_name_parm              = PipelineParameter(name='model_name', default_value= 'email_classifier')\n",
    "get_data_param_2             = PipelineParameter(name='get_data_param_2', default_value='value_2')\n",
    "get_data_param_3             = PipelineParameter(name='get_data_param_3', default_value='value_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "folder_name = 'batch-inferencing'\n",
    "script_folder = os.path.join(os.getcwd(), folder_name)\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/get_inferencing_data.py\n",
    "\n",
    "  \n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "#Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Get Inferencing Data\")\n",
    "parser.add_argument('--inference_data_location', type=str, required=True)\n",
    "parser.add_argument('--get_data_param_2', type=str, required=True)\n",
    "parser.add_argument('--get_data_param_3', type=str, required=True)\n",
    "parser.add_argument('--inferencing_dataset', dest='inferencing_dataset', required=True)\n",
    "\n",
    "# Note: the get_data_param args below are included only as an example of argument passing.\n",
    "# They are not consumed in the code sample shown here.\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "inference_data_location = args.inference_data_location\n",
    "get_data_param_2 = args.get_data_param_2\n",
    "get_data_param_3 = args.get_data_param_3\n",
    "inferencing_dataset = args.inferencing_dataset\n",
    "\n",
    "print(str(type(inferencing_dataset)))\n",
    "print(inferencing_dataset)\n",
    "\n",
    "#Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "#Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "#spam-data/spaminference.csv\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, inference_data_location + '/spaminference.csv'))\n",
    "\n",
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                name= 'email-tabular-dataset-raw',\n",
    "                                description='email data',\n",
    "                                tags = {'format':'csv'},\n",
    "                                create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "        print(ex)\n",
    "        \n",
    "df = tab_data_set.to_pandas_dataframe()\n",
    "\n",
    "print('dataset shape = ' + str(df.shape))\n",
    "print('saving inferencing data: ' + inferencing_dataset)\n",
    "\n",
    "# Save dataset for consumption in next pipeline step\n",
    "os.makedirs(inferencing_dataset, exist_ok=True)\n",
    "df.to_csv(os.path.join(inferencing_dataset, 'inferencing_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/score_inferencing_data.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.core.model import Model\n",
    "from azureml.data.datapath import DataPath\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import joblib\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "import time\n",
    "import pandas as pd\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "# Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Score Inferencing Data\")\n",
    "parser.add_argument('--model_name_parm', type=str, required=True)\n",
    "parser.add_argument('--scored_dataset', dest='scored_dataset', required=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "model_name = args.model_name_parm\n",
    "scored_dataset = args.scored_dataset\n",
    "\n",
    "# Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "# Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "# Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "\n",
    "inferencing_dataset = current_run.input_datasets['email_inferencing_data']\n",
    "inferencing_data_df = inferencing_dataset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "print('inferencing data df shape:' + str(inferencing_data_df.shape))\n",
    "\n",
    "#drop columns not in model\n",
    "#X = data['text']\n",
    "col_list = ['text']\n",
    "inferencing_data_df = inferencing_data_df[col_list]\n",
    "\n",
    "print('model_name' + model_name)\n",
    "\n",
    "# Get model from workspace - the code below will always retrieve the latest version of the model; specific versions can be targeted.\n",
    "model_list = Model.list(ws, name=model_name, latest=True)\n",
    "model_path = model_list[0].download(exist_ok=True)\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "print(inferencing_data_df.shape)\n",
    "\n",
    "\n",
    "X = inferencing_data_df['text']\n",
    "# Make predictions with new dataframe\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print('made predictions')\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "inferencing_data_df['Predictions']=predictions\n",
    "\n",
    "print(inferencing_data_df.head(5))\n",
    "\n",
    "\n",
    "# Save scored dataset\n",
    "os.makedirs(scored_dataset, exist_ok=True)\n",
    "print(scored_dataset)\n",
    "\n",
    "os.makedirs(scored_dataset, exist_ok=True)\n",
    "print(scored_dataset)\n",
    "inferencing_data_df.to_csv(os.path.join(scored_dataset, 'scored_data.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/publish_scored_data.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "# Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "# Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# Get inferencing dataset\n",
    "scored_dataset = current_run.input_datasets['email_scored_data']\n",
    "scored_data_df = scored_dataset.to_pandas_dataframe()\n",
    "\n",
    "# Save dataset to ./outputs dir\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "scored_data_df.to_csv(os.path.join('outputs', 'scored_data.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_inferencing_data_step = PythonScriptStep(\n",
    "    name='Get Inferencing Data',\n",
    "    script_name='get_inferencing_data.py',\n",
    "    arguments=[\n",
    "        '--inference_data_location', inference_data_location_parm,\n",
    "        '--get_data_param_2', get_data_param_2,\n",
    "        '--get_data_param_3', get_data_param_3,\n",
    "        '--inferencing_dataset', inferencing_dataset\n",
    "    ],\n",
    "    outputs=[inferencing_dataset],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=folder_name,\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "score_inferencing_data_step = PythonScriptStep(\n",
    "    name='Score Inferencing Data',\n",
    "    script_name='score_inferencing_data.py',\n",
    "    arguments=[\n",
    "        '--model_name_parm', model_name_parm,\n",
    "        '--scored_dataset', scored_dataset\n",
    "    ],\n",
    "    inputs=[inferencing_dataset.as_input(name= 'email_inferencing_data')],\n",
    "    outputs=[scored_dataset],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=folder_name,\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "publish_scored_data_step = PythonScriptStep(\n",
    "    name='Publish Scored Data',\n",
    "    script_name='publish_scored_data.py',\n",
    "    inputs=[scored_dataset.as_input(name= 'email_scored_data')],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=folder_name,\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline\n",
    "\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_inferencing_data_step, score_inferencing_data_step, publish_scored_data_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws,  '01-email-inference-pipeline')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish Pipeline\n",
    "\n",
    "Create a published version of pipeline that can be triggered via a REST API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_pipeline = pipeline.publish(name = 'Email Batch Inferencing Pipeline',\n",
    "#                                      description = 'Pipeline that generates batch predictions using a registered trained model.',\n",
    "#                                      continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest_endpoint = published_pipeline.endpoint\n",
    "# print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# interactive_auth = InteractiveLoginAuthentication()\n",
    "# auth_header = interactive_auth.get_authentication_header()\n",
    "# print('Authentication header ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# rest_endpoint = published_pipeline.endpoint\n",
    "# response = requests.post(rest_endpoint, \n",
    "#                          headers=auth_header, \n",
    "#                          json={\"ExperimentName\": user + \"rest-api-diabetes-batch\"})\n",
    "# run_id = response.json()[\"Id\"]\n",
    "# run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.pipeline.core.run import PipelineRun\n",
    "# from azureml.widgets import RunDetails\n",
    "\n",
    "# published_pipeline_run = PipelineRun(ws.experiments[user + \"rest-api-diabetes-batch\"], run_id)\n",
    "\n",
    "# # Block until the run completes\n",
    "# published_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
